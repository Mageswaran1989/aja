
\section{Reference Url}
 http://spark.apache.org/docs/latest/spark-standalone.html

\section {Notes}
$ cd /opt/spark/
$ ./sbin/start-master.sh
$ ifconfig
my ip: 115.242.144.22
Check Spark Master @ http://115.242.144.22:8080/ or @ http://localhost:8080/
$ ./sbin/start-slave.sh spark://localhost:7077


vim  /etc/spark/conf/spark-env.sh
export STANDALONE_SPARK_MASTER_HOST=`Aja`
export SPARK_MASTER_PORT spark://localhost:7077
#/ SPARK_MASTER_WEBUI_PORT and SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports
export SPARK_WORKER_CORES = 6 #, to set the number of cores to use on this machine
export SPARK_WORKER_MEMORY = 4 GB #, to set how much memory to use (for example: 1000 MB, 2 GB)
export SPARK_WORKER_INSTANCE = 4 #, to set the number of worker processes per node
export SPARK_WORKER_DIR, to set the working directory of worker processes



\section{Starting and Stopping Spark Standalone Clusters:}
To start Spark Standalone clusters:
1. On one host in the cluster, start the Spark Master:
   $ sudo service spark-master start
2. On all the other hosts, start the workers:
   $ sudo service spark-worker start
3. On one node, start the History Server:
   $ sudo service spark-history-server start

To stop Spark, use the following commands on the appropriate hosts:
$ sudo service spark-worker stop
$ sudo service spark-master stop
$ sudo service spark-history-server stop

\section{Configuring the Spark History Server}
Before you can run the Spark History Server, you must create the /user/spark/applicationHistory/ directory in HDFS and
set ownership and permissions as follows:
$ sudo -u hdfs hadoop fs -mkdir /user/spark
$ sudo -u hdfs hadoop fs -mkdir /user/spark/applicationHistory
$ sudo -u hdfs hadoop fs -chown -R spark:spark /user/spark
$ sudo -u hdfs hadoop fs -chmod 1777 /user/spark/applicationHistory

On Spark clients (systems from which you intend to launch Spark jobs), do the following:
1. Create /etc/spark/conf/spark-defaults.conf on the Spark client:
   $ cp /etc/spark/conf/spark-defaults.conf.template /etc/spark/conf/spark-defaults.conf
2. Add the following to /etc/spark/conf/spark-defaults.conf:
   spark.eventLog.dir=/user/spark/applicationHistory
   spark.eventLog.enabled=true

This causes Spark applications running on this client to write their history to the directory that the history server reads.
In addition, if you want the YARN ResourceManager to link directly to the Spark History Server, you can set the
spark.yarn.historyServer.address property in /etc/spark/conf/spark-defaults.conf:

spark.yarn.historyServer.address=http://HISTORY_HOST:HISTORY_PORT
