# Overview of Spark


## What happens when you run a Spark application? 
 
### Spark Cluster components  
![deploy](../../img/spark/deploy.png)

*Spark Cluster components from the Standalone vesrion:*  
1. Master node  
  - Runs Master daemon and manages all Worker Nodes  
2. Worker Nodes    
  - Runs Worker daemon which communicates with Master node and manages local Executors.    
3. Driver    
  - A application with SparkContext and main() that runs on a node becomes a Driver. This can be a locally launched one 
  or started from a Master node or that runs on a scheduled Worker node by a Yarn cluster.      
  - It is always better to start the Driver on the same network as the Worker node.    
 
Each Worker manages one or multiple ExecutorBackend processes. Each ExecutorBackend launches and manages an Executor 
instance. Each Executor maintains a thread pool, in which each task runs as a thread.
> Worker -> ExecutorBackend -> Execuotr -> Thread pool -> Tasks as a thread  

Each application has one Driver and multiple Executors. The tasks within the same Executor belong to the same application.

`Standalone Mode:`  
  - In Standalone deployment mode, ExecutorBackend is instantiated as CoarseGrainedExecutorBackend.  
  - Worker manages each CoarseGrainedExecutorBackend process thourgh an ExecutorRunner instance (Object).
   

#### Sample code:  
[GroupByTest.scala](../../../scala/tej/src/test/scala/org/aja/tej/test/spark/GroupByTest.scala) 
 
Let us visulaize the above code in our brain as
![UserView](../../img/spark/UserView.png) 
 
## Logical Plan

The actual execution procedure is more complicated than what we descrbed above. Generally speaking, Spark firstly creates a logical plan (namely data dependency graph) for each application, then it transforms the logical plan into a physical plan (a DAG graph of map/reduce stages and map/reduce tasks). After that, concrete map/reduce tasks will be lanuched to process the input data. Let's detail the logical plan of this application:

The function call of `RDD.toDebugString` can return the logical plan:

```scala
  MapPartitionsRDD[3] at groupByKey at GroupByTest.scala:51 (36 partitions)
    ShuffledRDD[2] at groupByKey at GroupByTest.scala:51 (36 partitions)
      FlatMappedRDD[1] at flatMap at GroupByTest.scala:38 (100 partitions)
        ParallelCollectionRDD[0] at parallelize at GroupByTest.scala:38 (100 partitions)
```
We can draw a diagram to illustrate logical plan:
![deploy](../../img/spark/JobRDD.png)

> Note that the **data in the partition** block only shows what data will be generated in this partition, but this does not mean that these data all reside in memory at the same time.

Let's detail the logical plan:
  - The user first initializes an array, which contains 100 integers (i.e., 0 to 99).
  - parallelize() generates the first RDD (ParallelCollectionRDD), in which each partititon contains an integer i.
  - FlatMappedRDD is generated by calling a transformation method (flatMap) on the ParallelCollectionRDD. Each partition of the FlatMappedRDD contains an `Array[(Int, Array[Byte])]`.
  - The first count() performs on FlatMappedRDD.
  - Since the FlatMappedRDD is cached in memory, its partitions are colored differently.
  - groupByKey() generates the following 2 RDDs (ShuffledRDD and MapPartitionsRDD), and we will see the reason in later chapters.
  - ShuffleRDD in the logical plan means that the job needs a shuffle. This shuffle is similar with the shuffle in Hadoop MapReduce.
  - MapPartitionRDD contains groupByKey()'s computing results.
  - Each value in MapPartitionRDD (`Array[Byte]`) is converted to `Iterable`.
  - The last count() action performs on MapPartitionRDD.

**The logical plan represents the applicaion dataflow, including the data transformations , the intermediate RDDs, and the data dependency between these RDDs.**

## Physical Plan

The logical plan aims to model the dataflow, not the execution flow. The dataflow and execution flow are unified in Hadoop. In Hadoop, the dataflow is pre-defined and fixed, users just need to write map() and reduce() functions. The map/reduce tasks have fixed processing steps. However in Spark, the dataflow is very flexible and could be very complicated, so it's difficult to simply combine the dataflow and execution flow together. For this reason, Spark separates the dataflow from the actual task execution, and has algorithms to transform a logical plan into a physical plan. We'll discuss this transformation in later chapters.

For the example application, let's draw its physical DAG:
![deploy](../../img/spark/PhysicalView.png)

We can see that the GroupByTest application generates 2 Spark jobs, the first job is triggered by the first action (i.e., `pairs.count()`). Let's detail this first job:

  - This job contains only 1 stage, which has 100 ResultTasks. Here, the stage is similar with the map stage in Hadoop but not shown in this figure. The stage concept will be detailed in later chapters.
  - Each task performs flatMap, generates FlatMappedRDD, and then executes the action `count()` to count the record number in each partition. For example, partition 99 has  9 records, so the `result 99` is 9.
  - Since `pairs1` is specified to be cached, the tasks will cache the partitions of FlatMappedRDD inside the executor's memory space at runtime.
  - After the ResultTasks finish, the driver collects the tasks' results and sums them up.
  - Job 0 completes.

The second job is triggered by `pairs1.groupByKey(numReducers).count`:

  - This job has 2 stages.
  - Stage 0 contains 100 ShuffleMapTask, each task reads a partition of `paris` from the cache, repartitions it, and then write the repartitioned results into local disk. This step is similar to partitioning map outputs in Hadoop.
  - Stage 1 contains 36 ResultTasks. Each task fetches and shuffles the data that it needs to process. It fetches the data, aggregates the data, and performes mapPartitions() operation in a pipeline style. Finally, count() is applied to get the result.
  - After the ResultTasks finish, the driver collects the tasks' results and sum them up.
  - Job 1 completes.

We can see that the physical plan is not simple. A Spark application can contain multiple jobs, each job could have multiple stages, and each stage has multiple tasks. **In later chapters, we'll detail how the jobs, stages and tasks are generated**
