\documentclass{wileysev}
\usepackage{graphicx}
\usepackage{w-bookps}
\usepackage{neuralnetwork}
\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usepackage{mathtools}
%\usepackage{amsmath}
\usepackage{float}
\floatstyle{boxed}
\restylefloat{figure}



\begin{document}


\booktitle{Hopfield Network}

\authors{Mageswaran\\
\affil{Aja}
}

\offprintinfo{Hopfield Network, First Edition}{Mageswaran}

%% Can use \\ if title, and edition are too wide, ie,
%% \offprintinfo{Survey Methodology,\\ Second Edition}{Robert M. Groves}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
\titlepage



%http://www.comp.leeds.ac.uk/ai23/reading/Hopfield.pdf



\chapter{What is Hopfield?}
\chapterauthors{Mageswaran.D
\chapteraffil{Karunya University}
}

\section{Introduction}

\def\layersep{2.5cm}
\begin{figure}[h!]
\caption{An artificial neuron as used in a Hopfield network} 
\centering

\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
%    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=black!50];
    \tikzstyle{output neuron}=[neuron, fill=black!50];
    \tikzstyle{hidden neuron}=[neuron, fill=black!50];
    \tikzstyle{annot} = [text width=4em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y in {1,...,3}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:Input \#\y] (I-\name) at (0,-\y) {};

    % Draw the output layer node
    \node[output neuron,pin={[pin edge={->}]right:Output}, right of=I-2] (O) {};

    % Connect every node in the input layer with the output layer
    \foreach \source in {1,...,3}
        \path (I-\source) edge (O);

    % Annotate the layers
    \node[annot,below of=I-2] {Input layer};
    \node[annot,below of=O] {Output layer};

\end{tikzpicture}

%	\begin{neuralnetwork}[height=4]
%		\newcommand{\nodetextclear}[2]{}
%		\newcommand{\nodetextx }[2]{$x_#2$}
%		\newcommand{\nodetexty}[2]{$y_#2$}
%		\inputlayer[count=3, bias=false, title=Input\\layer, text=\nodetextx]
%		\outputlayer[count=1, title=Output\\layer, text=\nodetexty] 
%		\linklayers
%	\end{neuralnetwork}

\end{figure}

Hopfield networks are constructed from artificial neurons (see Fig. 1). These
artificial neurons have N inputs. With each input $i$ there is a weight $w_i$ associated.
They also have an output. The state of the output is maintained, until
the neuron is updated. Updating the neuron entails the following operations:
	
\begin{itemize}
\item The value of each input, $x_i$ is determined and the weighted sum of all inputs,\vspace{2mm}\sum\limits_{i=1}^n $w_i$ $x_i$ is calculated.
\item The output state of the neuron is set to $+1$ if the weighted input sum is
larger or equal to $0$. It is set to $-1$ if the weighted input sum is smaller
than $0$.
\item A neuron retains its output state until it is updated again.
\end{itemize}

Written as formula: \\*
\[ o = \left\{ 
  \begin{array}{l l}
    \hspace{3 mm} 1  : & \quad \sum_{i} w_ix_i >= 0 \\
    \hspace{2 mm}-1 : & \quad \sum_{i} w_ix_i < 0
  \end{array} \right.\]
  
  
A Hopfield network is a network of $N$ such artificial neurons, which are fully
connected. The connection weight from neuron $j$ to neuron $i$ is given by a
number $w_ij$. The collection of all such numbers is represented by the weight
matrix $W$, whose components are $w_ij$. \\
Now given the weight matrix and the updating rule for neurons the dynamics
of the network is defined if we tell in which order we update the neurons. There
are two ways of updating them:

\begin {itemize}
\item Asynchronous: one picks one neuron, calculates the weighted input sum
and updates immediately. This can be done in a fixed order, or neurons
can be picked at random, which is called asynchronous random updating.
\item Synchronous: the weighted input sums of all neurons are calculated without
updating the neurons. Then all neurons are set to their new value,
according to the value of their weighted input sum. The lecture slides
contain an explicit example of synchronous updating.
\end{itemize}


\end{document}
