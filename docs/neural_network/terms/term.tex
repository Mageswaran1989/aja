\documentclass[12pt, right open]{memoir}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows,automata}
\usetikzlibrary{shapes.geometric, calc, intersections}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{float}
\floatstyle{boxed}
\restylefloat{figure}
\usepackage{multirow} %for tables
\usepackage{ifthen}
\setcounter{secnumdepth}{5}

%To reduce vetical line spaces between list items
\usepackage{enumitem}
\setlist{nolistsep,leftmargin=*}

\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
%  Foo bar & \specialcell{Foo\\bar} & Foo bar \\    % vertically centered
%Foo bar & \specialcell[t]{Foo\\bar} & Foo bar \\ % aligned with top rule
%Foo bar & \specialcell[b]{Foo\\bar} & Foo bar \\ % aligned with bottom rule

\newcommand{\matplus}{
~~
  }

\begin{document}

%http://www.comp.leeds.ac.uk/ai23/reading/Hopfield.pdf

% >>>>>>>> Tikz Style sheet
\tikzstyle{every pin edge}=[<-,shorten <=1pt]
\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
\tikzstyle{input neuron}=[neuron, fill=black!50]
\tikzstyle{output neuron}=[neuron, fill=black!50]
\tikzstyle{hidden neuron}=[neuron, fill=black!50]
\tikzstyle{annot} = [text width=4em, text centered]

\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black]
\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black]
\tikzstyle{arrow} = [thick,->,>=stealth]
% <<<<<<<< Tikz Style sheet

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Common Terms to Start with Neural Network}
\section{Neural Network}
A neural network is a massively parallel distributed processor made up of simple processing units, which has a natural propensity for storing experimental knowledge and making it available for use. It resembles the brain in two aspects:
\begin{enumerate}
\item Knowledge is acquired by the network from its environment through a learning process.
\item Inter-neuron connections strengths, known as synaptic weights, are used to store the acquired knowledge. 
\end{enumerate} 
\section{Stability for a Neural Network}
Stability refers to such convergence that facilitates an end to the iterative
process. For example, if any two consecutive cycles result in the same output
for the network, then there may be no need to do more iterations. In this case,
convergence has occurred, and the network has stabilized in its operation. If
weights are being modified after each cycle, then convergence of weights
would constitute stability for the network.
In some situations, it takes many more iterations than you desire, to have
output in two consecutive cycles to be the same. Then a tolerance level on the
convergence criterion can be used. With a tolerance level, you accomplish
early but satisfactory termination of the operation of the network.

\section{Plasticity for a Neural Network}
Suppose a network is trained to learn some patterns, and in this process the
weights are adjusted according to an algorithm. After learning these patterns
and encountering a new pattern, the network may modify the weights in order
to learn the new pattern. But what if the new weight structure is not responsive
to the new pattern? Then the network does not possess plasticity—the ability
to deal satisfactorily with new short-term memory (STM) while retaining
long-term memory (LTM). Attempts to endow a network with plasticity may
have some adverse effects on the stability of your network.

Plasticity permits the developing nervous system to adapt to its surrounding environment.
\section{Short-Term Memory and Long-Term Memory}
We alluded to short-term memory (STM) and long-term memory (LTM) in the
previous paragraph. STM is basically the information that is currently and
perhaps temporarily being processed. It is manifested in the patterns that the
network encounters. LTM, on the other hand, is information that is already
stored and is not being currently processed. In a neural network, STM isusually characterized by patterns and LTM is characterized by the
connections’ weights. The weights determine how an input is processed in the
network to yield output. During the cycles of operation of a network, the
weights may change. After convergence, they represent LTM, as the weight
levels achieved are stable.

\section{Generalization}
It refers to the ability of neural networks to produce reasonable outputs for inputs not encountered during the training.

\section{Learning Algorithm}
The procedure used to perform the learning process is called a learning algorithm, the function of which is to modify the synaptic weights of the network in an orderly fashion to attain the design objective.

\end{document}